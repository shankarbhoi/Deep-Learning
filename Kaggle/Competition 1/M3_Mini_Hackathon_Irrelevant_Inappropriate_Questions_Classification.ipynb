{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hNgLag1Euy3H"
      },
      "source": [
        "# Advanced Programme in Deep Learning (Foundations and Applications)\n",
        "## A Program by IISc and TalentSprint\n",
        "\n",
        "### Mini Project Notebook: Irrelevant/inappropriate Questions Classification using Deep Neural Networks.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "maritime-miami"
      },
      "source": [
        "## Learning Objectives"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nljJR6CwfZN_"
      },
      "source": [
        "At the end of the mini-hackathon, you will be able to :\n",
        "\n",
        "* perform data preprocessing/preprocess the text\n",
        "* represent the text/words using the pretrained word embeddings - Word2Vec/Glove\n",
        "* build the deep neural networks to classify the questions as Irrelevant/inappropriate or not\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FL0Ve1abn6YJ"
      },
      "source": [
        "## Dataset\n",
        "\n",
        "The challenge in this competition is to predict whether a question asked on a well known public forum/platform is irrelevant/inappropriate or not.\n",
        "\n",
        "A irrelevant/inappropriate question is defined as a question intended to make a statement and not with a purpose of looking for helpful/meaningful answers. The following are some of the characteristics that can signify that a question is irrelevant/inappropriate:\n",
        "\n",
        "* Based on false information, or contains absurd assumptions\n",
        "* Does not have a non-neutral tone\n",
        "* Has an exaggerated tone to underscore a point about a group of people\n",
        "* Is rhetorical and meant to imply a statement about a group of people\n",
        "* Is disparaging or inflammatory against an individual or a group of people\n",
        "* Uses sexual content (such as incest, pedophilia), and not to seek genuine answers\n",
        "* Suggests a discriminatory idea against a protected class of people, or seeks confirmation of a stereotype\n",
        "* Based on an unrealistic premise about a group of people\n",
        "* Is not grounded in reality\n",
        "\n",
        "The training dataset includes the questions 1044897 that was asked, and whether it was identified as irrelevant/inappropriate (target = 1) or as relevant/appropriate (target = 0). The test dataset consists of approximately 261000 questions.\n",
        "\n",
        "The training data might be imbalanced or noisy. They are not guaranteed to be perfect. Please take the necessary actions/steps while building the model.\n",
        " \n",
        "\n",
        "## Description\n",
        "\n",
        "This dataset has the following information:\n",
        "\n",
        "1. **qid** - unique question identifier\n",
        "2. **question_text** - the text of the question asked in the well known public forum/platform\n",
        "3. **target** - a question labeled \"irrelevant/inappropriate\" has a value of 1, otherwise 0\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ih-oasWmdZul"
      },
      "source": [
        "## Problem Statement"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qfWGmjNHdZul"
      },
      "source": [
        "To perform classification of approximately 261000 questions asked on a well known public form using Deep Neural Networks such as RNN/CNN/BERT/LSTM as 'irrelevant/inappropriate' questions or 'relevant/appropriate' questions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1BQEA97zTlTa"
      },
      "source": [
        "## Grading = 10 Marks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DdYmy-tJgURN"
      },
      "source": [
        "Here is a handy link to Kaggle's competition documentation (https://www.kaggle.com/docs/competitions), which includes, among other things, instructions on submitting predictions (https://www.kaggle.com/docs/competitions#making-a-submission)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y8OapRtHgLnU"
      },
      "source": [
        "## Instructions for downloading train and test dataset from Kaggle API are as follows:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DO2jS73oLnCR"
      },
      "source": [
        "### 1. Create an API key in Kaggle.\n",
        "\n",
        "To do this, go to the competition site on Kaggle at (https://www.kaggle.com/t/bde6f23028154933a99e4b4ca8a3dff2) and click on user then click on your profile as shown below. Click Account.\n",
        "\n",
        "![alt text](https://cdn.iisc.talentsprint.com/DLFA/Experiment_related_data/Capture-NLP.PNG)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DkzGffHdbwX2"
      },
      "source": [
        "### 2. Next, scroll down to the API access section and click on **Create New Token** to download an API key (kaggle.json). \n",
        "\n",
        "![alt text](https://cdn.iisc.talentsprint.com/DLFA/Experiment_related_data/Capture-NLP_1.PNG)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WtETuXx8b-OC"
      },
      "source": [
        "### 3. Upload your kaggle.json file using the following snippet in a code cell:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-1pfXBDxWl0Y"
      },
      "source": [
        "from google.colab import files\n",
        "files.upload()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GCV_T6MMW4eX"
      },
      "source": [
        "#If successfully uploaded in the above step, the 'ls' command here should display the kaggle.json file.\n",
        "%ls"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JbukdzJ6cE32"
      },
      "source": [
        "### 4. Install the Kaggle API using the following command\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dMj1n1MJcqzN"
      },
      "source": [
        "!pip install -U -q kaggle==1.5.8"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Vpy9P1nchhd"
      },
      "source": [
        "### 5. Move the kaggle.json file into ~/.kaggle, which is where the API client expects your token to be located:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yQbPsDOLZ0b4"
      },
      "source": [
        "!mkdir -p ~/.kaggle\n",
        "!cp kaggle.json ~/.kaggle/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BenAWlpI73sm"
      },
      "source": [
        "# Execute the following command to verify whether the kaggle.json is stored in the appropriate location: ~/.kaggle/kaggle.json\n",
        "!ls ~/.kaggle"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vm2jGsCradOS"
      },
      "source": [
        "!chmod 600 /root/.kaggle/kaggle.json # run this command to ensure your Kaggle API token is secure on colab"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "32unPZKzdI72"
      },
      "source": [
        "### 6. Now download the Test Data from Kaggle"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ppuy5gRKHFwv"
      },
      "source": [
        "**NOTE: If you get a '404 - Not Found' error after running the cell below, it is most likely that the user (whose kaggle.json is uploaded above) has not 'accepted' the rules of the competition and therefore has 'not joined' the competition.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "41-ETZCE_A1j"
      },
      "source": [
        "If you encounter **401-unauthorised** download latest **kaggle.json** by repeating steps 1 & 2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TY40TmgfHq0s"
      },
      "source": [
        "#If you get a forbidden link, you have most likely not joined the competition.\n",
        "!kaggle competitions download -c toxic-questions-classification"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip /content/toxic-questions-classification.zip"
      ],
      "metadata": {
        "id": "mvKRiFNglvpC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## YOUR CODING STARTS FROM HERE"
      ],
      "metadata": {
        "id": "QeKon2vruI_c"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "abstract-stocks"
      },
      "source": [
        "## Import required packages"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.utils import pad_sequences\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Embedding, LSTM, SpatialDropout1D\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.utils.np_utils import to_categorical\n",
        "from sklearn.utils import resample\n",
        "from sklearn.utils import shuffle\n",
        "from sklearn.metrics import confusion_matrix,classification_report\n",
        "import re\n",
        "\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords # to get collection of stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "import string\n",
        "import gensim\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "O5RcxwQUku6x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "53g0zVbjRV7K"
      },
      "source": [
        "##   **Stage 1**:  Data Loading and Perform Exploratory Data Analysis (1 Points)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data = pd.read_csv('train_dataset.csv')\n",
        "data_t = pd.read_csv('test_dataset.csv')"
      ],
      "metadata": {
        "id": "vq7__byEHabv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.head()"
      ],
      "metadata": {
        "id": "vhkVsTiXOThb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.groupby('target').count()"
      ],
      "metadata": {
        "id": "gvehWZ9OOqx8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.groupby('target').target.count().plot.bar()"
      ],
      "metadata": {
        "id": "2hnWVcMrOrZ_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "max(data['question_text'].str.len()), min(data['question_text'].str.len())"
      ],
      "metadata": {
        "id": "r1xz2bSMOrg7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data[data['question_text'].str.len() <= 10]"
      ],
      "metadata": {
        "id": "2ABEFkAMOrl_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3oLyIb19KcdL"
      },
      "source": [
        "##   **Stage 2**: Data Pre-Processing  (1 Points)\n",
        "\n",
        "####  Clean and Transform the data into a specified format\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Data Preprocessing \n",
        "def preprocess_text(sen):\n",
        "    \n",
        "    sen = re.sub('<.*?>', ' ', sen) # remove html tag\n",
        "\n",
        "    tokens = word_tokenize(sen)  # tokenizing words\n",
        "\n",
        "    tokens = [w.lower() for w in tokens]    # lower case\n",
        "\n",
        "    table = str.maketrans('', '', string.punctuation)  # remove punctuations\n",
        "    stripped = [w.translate(table) for w in tokens]\n",
        "\n",
        "    words = [word for word in stripped if word.isalpha()]  # remove non alphabet\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "\n",
        "    words = [w for w in words if not w in stop_words]   # remove stop words\n",
        "    \n",
        "    #words = [w for w in words if len(w) > 2]  # Ignore words less than 2\n",
        "    \n",
        "    return words"
      ],
      "metadata": {
        "id": "-6ZCiIxxKiq7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x=preprocess_text('Shankar is <> looking for a job in 90 days.')\n",
        "x"
      ],
      "metadata": {
        "id": "0GVMHyBaUVk5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_lines = data\n",
        "data_lines['question_text'] = data_lines.apply(lambda x: preprocess_text(x['question_text']), axis=1)\n"
      ],
      "metadata": {
        "id": "_DHqxK5SVVKi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_lines.head()"
      ],
      "metadata": {
        "id": "Lxd-WhZDaNP3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1jZg7yL2TtTM"
      },
      "source": [
        "##   **Stage 3**: Build the Word Embeddings using pretrained Word2vec/Glove (Text Representation) (1 Point)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "EMBEDDING_DIM = 100\n",
        "# Train word2vec model after preprocessing the reviews\n",
        "model = gensim.models.Word2Vec(sentences=data_lines[\"question_text\"], vector_size=EMBEDDING_DIM, window=5, workers=4, min_count=1)"
      ],
      "metadata": {
        "id": "_9rTpuiwSy0S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(model)"
      ],
      "metadata": {
        "id": "zJia-8MibZZV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "words = list(model.wv.index_to_key)\n",
        "print('Vocabulary size: %d' % len(words))"
      ],
      "metadata": {
        "id": "XAAfw2v9bZ6A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save model\n",
        "filename = \"questions_embedding_word2vec.txt\"\n",
        "model.wv.save_word2vec_format(filename, binary=False)"
      ],
      "metadata": {
        "id": "GFSng5KSbhOR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os \n",
        "\n",
        "embeddings_index = {}\n",
        "f = open(os.path.join('','questions_embedding_word2vec.txt'), encoding=\"utf-8\")\n",
        "\n",
        "for line in f:\n",
        "    values = line.split()\n",
        "    word = values[0]\n",
        "    coefs = np.asarray(values[1:])\n",
        "    embeddings_index[word] = coefs\n",
        "f.close()"
      ],
      "metadata": {
        "id": "QtvmR9aVbyQN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embeddings_index[\"bjp\"]"
      ],
      "metadata": {
        "id": "LaohAeD6cDI1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I6jfm3YFUL7i"
      },
      "source": [
        "##   **Stage 4**: Build and Train the Deep networks model using Pytorch/Keras (5 Points)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(data_lines[\"question_text\"])\n",
        "X = tokenizer.texts_to_sequences(data_lines[\"question_text\"])\n",
        "\n",
        "X = pad_sequences(X, padding='post', truncating='post')\n",
        "Y = data_lines[\"target\"]\n",
        "\n",
        "print('Shape of X tensor:', X.shape)\n",
        "print('Shape of Y tensor', Y.shape)"
      ],
      "metadata": {
        "id": "Dz1-Bs4pUdsf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, Y_train, Y_test = train_test_split(X,Y, test_size = 0.20, random_state = 42)\n",
        "print(X_train.shape,Y_train.shape)\n",
        "print(X_test.shape,Y_test.shape)"
      ],
      "metadata": {
        "id": "LmffNHirlZSz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Adding 1 because of reversed 0 index\n",
        "word_index = tokenizer.word_index\n",
        "vocab_size = len(word_index) + 1\n",
        "\n",
        "# Create a weight matrix for words in the training data\n",
        "embedding_matrix = np.zeros((vocab_size, EMBEDDING_DIM))\n",
        "\n",
        "for word, index in word_index.items():\n",
        "  embedding_vector = embeddings_index.get(word)\n",
        "  if embedding_vector is not None:\n",
        "    # If words not found in embedding matrix will be all 0's\n",
        "    embedding_matrix[index, :] = embedding_vector"
      ],
      "metadata": {
        "id": "3_GmpGVal7Na"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define Model\n",
        "model = Sequential()\n",
        "embedding_layer = Embedding(vocab_size,\n",
        "                            EMBEDDING_DIM, \n",
        "                            weights = [embedding_matrix],\n",
        "                            trainable=False)\n",
        "model.add(embedding_layer)\n",
        "model.add(LSTM(32, dropout=0.3, recurrent_dropout=0.2))\n",
        "model.add(Dense(1, activation='sigmoid'))"
      ],
      "metadata": {
        "id": "WKj64MCnmqOS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(model.summary())"
      ],
      "metadata": {
        "id": "bHwtBaEInIu9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])"
      ],
      "metadata": {
        "id": "4jjTeOoWnLAh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train.shape, Y_train.shape, X_test.shape, Y_test.shape"
      ],
      "metadata": {
        "id": "1_jTxjzbnmbS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(X_train, Y_train, batch_size=1280, verbose=1, epochs=5, validation_split=0.2)"
      ],
      "metadata": {
        "id": "JSX7j-O0n4RL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "acc_train = history.history['accuracy']\n",
        "acc_val = history.history['val_accuracy']\n",
        "epochs = range(1,6)\n",
        "plt.plot(epochs, acc_train, 'g', label='Training accuracy')\n",
        "plt.plot(epochs, acc_val, 'b', label='validation accuracy')\n",
        "plt.title('Training and Validation Accuracy')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "jlX12YEFn4wq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "# save the model to disk\n",
        "filename = 'finalized_model.sav'\n",
        "pickle.dump(model, open(filename, 'wb'))\n",
        "# load the model from disk\n",
        "#loaded_model = pickle.load(open(filename, 'rb'))"
      ],
      "metadata": {
        "id": "dnIwIuWjrcpE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.evaluate(X_test, Y_test)"
      ],
      "metadata": {
        "id": "Nrdl6BB0pUtf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k-O0Jx99UhmI"
      },
      "source": [
        "##   **Stage 5**: Evaluate the Model and get model predictions on the test dataset (2 Points)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jp3eI2i-nJxs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# YOUR CODE HERE"
      ],
      "metadata": {
        "id": "PSaAlhGGUitF"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}